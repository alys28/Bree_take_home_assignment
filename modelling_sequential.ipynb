{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this approach, I am using an LSTM, as it is more reliable and effective than an RNN, and more easily scalable than transformers, with faster training and inference time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.load('preprocessed_data/50_data_pts_features_sequential.npy')\n",
    "y = np.load('preprocessed_data/50_data_pts_labels_sequential.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Shuffle the training data after splitting, to prevent data leakage\n",
    "train_permutation = np.random.permutation(len(X_train))\n",
    "X_train = X_train[train_permutation]\n",
    "y_train = y_train[train_permutation]\n",
    "\n",
    "# Shuffle the testing data\n",
    "test_permutation = np.random.permutation(len(X_test))\n",
    "X_test = X_test[test_permutation]\n",
    "y_test = y_test[test_permutation]\n",
    "print(\"X_train:\\n\", X_train)\n",
    "print(\"y_train:\\n\", y_train)\n",
    "print(\"X_test:\\n\", X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((348, 43, 18), (348,), (150, 43, 18), (150,))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape after normalization: torch.Size([348, 43, 18])\n",
      "Validation data shape after normalization: torch.Size([150, 43, 18])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "X_train_reshaped = X_train.reshape(-1, 18)  # Reshape to (348 * 43, 18)\n",
    "X_val_reshaped = X_test.reshape(-1, 18)  # Reshape to (150 * 43, 18) --> For normalization\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_normalized = scaler.fit_transform(X_train_reshaped) \n",
    "X_val_normalized = scaler.transform(X_val_reshaped)  # Only transform on validation data, no fitting\n",
    "\n",
    "# Reshape back to the original 3D shape\n",
    "X_train_normalized = X_train_normalized.reshape(348, 43, 18)\n",
    "X_val_normalized = X_val_normalized.reshape(150, 43, 18)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(X_val_normalized, dtype=torch.float32)\n",
    "\n",
    "print(\"Training data shape after normalization:\", X_train_tensor.shape)\n",
    "print(\"Validation data shape after normalization:\", X_val_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training targets shape after normalization: torch.Size([348, 1])\n",
      "Validation targets shape after normalization: torch.Size([150, 1])\n"
     ]
    }
   ],
   "source": [
    "target_scaler = StandardScaler()\n",
    "\n",
    "y_train_normalized = target_scaler.fit_transform(y_train.reshape(-1, 1)).flatten()  # Normalize training targets\n",
    "y_val_normalized = target_scaler.transform(y_test.reshape(-1, 1)).flatten()  # Normalize validation targets\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "y_train_tensor = torch.tensor(y_train_normalized, dtype=torch.float32).view(-1, 1)\n",
    "y_val_tensor = torch.tensor(y_val_normalized, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "print(\"Training targets shape after normalization:\", y_train_tensor.shape)\n",
    "print(\"Validation targets shape after normalization:\", y_val_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/100], Train Loss: 1.0025, Val Loss: 0.3170\n",
      "Epoch [20/100], Train Loss: 1.0006, Val Loss: 0.3281\n",
      "Epoch [30/100], Train Loss: 1.0000, Val Loss: 0.3221\n",
      "Epoch [40/100], Train Loss: 1.0000, Val Loss: 0.3228\n",
      "Epoch [50/100], Train Loss: 1.0000, Val Loss: 0.3242\n",
      "Epoch [60/100], Train Loss: 1.0000, Val Loss: 0.3228\n",
      "Epoch [70/100], Train Loss: 1.0000, Val Loss: 0.3236\n",
      "Epoch [80/100], Train Loss: 1.0000, Val Loss: 0.3233\n",
      "Epoch [90/100], Train Loss: 1.0000, Val Loss: 0.3233\n",
      "Epoch [100/100], Train Loss: 1.0000, Val Loss: 0.3234\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer to output a single value\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through LSTM\n",
    "        lstm_out, (hn, cn) = self.lstm(x)\n",
    "        # Get the output of the last time step (last hidden state)\n",
    "        last_hidden_state = lstm_out[:, -1, :]\n",
    "        # Pass through the fully connected layer\n",
    "        output = self.fc(last_hidden_state)\n",
    "        # Scalar value\n",
    "        return output\n",
    "\n",
    "input_size = 18  # Number of features (dimensionality of each time step)\n",
    "hidden_size = 64  # Number of LSTM units (hidden state dimension)\n",
    "output_size = 1\n",
    "num_layers = 2  # Number of LSTM layers\n",
    "\n",
    "model = LSTMModel(input_size=input_size, hidden_size=hidden_size, output_size=output_size, num_layers=num_layers)\n",
    "\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss for regression, bigger penalty for large errors, which we want to prevent for this task, otherwise we run larger risks of losing revenue\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 100  # Set the number of epochs\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  \n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train_tensor)\n",
    "    train_loss = criterion(outputs, y_train_tensor)\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "    model.eval()  \n",
    "    with torch.no_grad(): \n",
    "        val_outputs = model(X_val_tensor)\n",
    "        val_loss = criterion(val_outputs, y_val_tensor)\n",
    "    if (epoch+1) % 10 == 0:\n",
    "        print(f\"poch [{epoch+1}/{num_epochs}], Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given that these are just test runs, it is not appropriate to make conclusions as to which method is more effective. However, given the results and assuming that these are legitimate results, then choosing the LSTM would be a better choice since the loss on the test set is better than non-sequential models. However, a much more rigorous testing and validation process would be needed, such as K-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
